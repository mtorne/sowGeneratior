/**
 * Generative AI Service Inference API
 * OCI Generative AI is a fully managed service that provides a set of state-of-the-art, customizable large language models (LLMs) that cover a wide range of use cases for text generation, summarization, and text embeddings.

Use the Generative AI service inference API to access your custom model endpoints, or to try the out-of-the-box models to {@link #eNGenerative-ai-inferenceLatestChatResultChat(ENGenerative-ai-inferenceLatestChatResultChatRequest) eNGenerative-ai-inferenceLatestChatResultChat}, {@link #eNGenerative-ai-inferenceLatestGenerateTextResultGenerateText(ENGenerative-ai-inferenceLatestGenerateTextResultGenerateTextRequest) eNGenerative-ai-inferenceLatestGenerateTextResultGenerateText}, {@link #eNGenerative-ai-inferenceLatestSummarizeTextResultSummarizeText(ENGenerative-ai-inferenceLatestSummarizeTextResultSummarizeTextRequest) eNGenerative-ai-inferenceLatestSummarizeTextResultSummarizeText}, and {@link #eNGenerative-ai-inferenceLatestEmbedTextResultEmbedText(ENGenerative-ai-inferenceLatestEmbedTextResultEmbedTextRequest) eNGenerative-ai-inferenceLatestEmbedTextResultEmbedText}.

To use a Generative AI custom model for inference, you must first create an endpoint for that model. Use the {@link #eNGenerative-aiLatest(ENGenerative-aiLatestRequest) eNGenerative-aiLatest} to {@link #eNGenerative-aiLatestModel(ENGenerative-aiLatestModelRequest) eNGenerative-aiLatestModel} by fine-tuning an out-of-the-box model, or a previous version of a custom model, using your own data. Fine-tune the custom model on a {@link #eNGenerative-aiLatestDedicatedAiCluster(ENGenerative-aiLatestDedicatedAiClusterRequest) eNGenerative-aiLatestDedicatedAiCluster}. Then, create a {@link #eNGenerative-aiLatestDedicatedAiCluster(ENGenerative-aiLatestDedicatedAiClusterRequest) eNGenerative-aiLatestDedicatedAiCluster} with an {@link Endpoint} to host your custom model. For resource management in the Generative AI service, use the {@link #eNGenerative-aiLatest(ENGenerative-aiLatestRequest) eNGenerative-aiLatest}.

To learn more about the service, see the [Generative AI documentation](https://docs.oracle.com/iaas/Content/generative-ai/home.htm).

 * OpenAPI spec version: 20231130
 *
 *
 * NOTE: This class is auto generated by OracleSDKGenerator.
 * Do not edit the class manually.
 *
 * Copyright (c) 2020, 2025, Oracle and/or its affiliates.  All rights reserved.
 * This software is dual-licensed to you under the Universal Permissive License (UPL) 1.0 as shown at https://oss.oracle.com/licenses/upl or Apache License 2.0 as shown at http://www.apache.org/licenses/LICENSE-2.0. You may choose either license.
 */
import * as model from "../model";
/**
 * The response to the chat conversation.
 */
export interface CohereChatResponse extends model.BaseChatResponse {
    /**
     * Contents of the response that the model generates.
     */
    "text": string;
    /**
     * The list of previous messages between the user and the model. The chat history gives the model context for responding to the user's inputs.
     */
    "chatHistory"?: Array<model.CohereMessage>;
    /**
     * Inline citations for the generated response.
     */
    "citations"?: Array<model.Citation>;
    /**
     * If set to true, a search for documents is required.
     */
    "isSearchRequired"?: boolean;
    /**
     * Why the generation stopped.
     */
    "finishReason": CohereChatResponse.FinishReason;
    /**
     * If there is an error during the streaming scenario, then the {@code errorMessage} parameter contains details for the error.
     */
    "errorMessage"?: string;
    /**
     * The generated search queries.
     */
    "searchQueries"?: Array<model.SearchQuery>;
    /**
      * The documents that the model can refer to when generating a response. Each document is a JSON string that represents the field and values of the document.
  * <p>
  Example:
  * '[
  *   {
  *     \"id\": \"doc_0\",
  *     \"snippet\": \"Emperor penguins are the tallest.\",
  *     \"title\": \"Tall penguins\"
  *   },
  *   {
  *     \"id\": \"doc_1\",
  *     \"snippet\": \"Emperor penguins only live in Antarctica.\",
  *     \"title\": \"Penguin habitats\"
  *   }
  * ]'
  *
      */
    "documents"?: Array<any>;
    /**
     * A list of tool calls generated by the model.
     */
    "toolCalls"?: Array<model.CohereToolCall>;
    /**
     * The full prompt that was sent to the model if isEcho is true when request.
     */
    "prompt"?: string;
    "usage"?: model.Usage;
    "apiFormat": string;
}
export declare namespace CohereChatResponse {
    enum FinishReason {
        Complete = "COMPLETE",
        ErrorToxic = "ERROR_TOXIC",
        ErrorLimit = "ERROR_LIMIT",
        Error = "ERROR",
        UserCancel = "USER_CANCEL",
        MaxTokens = "MAX_TOKENS",
        /**
         * This value is used if a service returns a value for this enum that is not recognized by this
         * version of the SDK.
         */
        UnknownValue = "UNKNOWN_VALUE"
    }
    function getJsonObj(obj: CohereChatResponse, isParentJsonObj?: boolean): object;
    const apiFormat = "COHERE";
    function getDeserializedJsonObj(obj: CohereChatResponse, isParentJsonObj?: boolean): object;
}
