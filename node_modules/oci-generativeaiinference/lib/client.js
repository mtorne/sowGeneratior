"use strict";
/**
 * Generative AI Service Inference API
 * OCI Generative AI is a fully managed service that provides a set of state-of-the-art, customizable large language models (LLMs) that cover a wide range of use cases for text generation, summarization, and text embeddings.

Use the Generative AI service inference API to access your custom model endpoints, or to try the out-of-the-box models to {@link #eNGenerative-ai-inferenceLatestChatResultChat(ENGenerative-ai-inferenceLatestChatResultChatRequest) eNGenerative-ai-inferenceLatestChatResultChat}, {@link #eNGenerative-ai-inferenceLatestGenerateTextResultGenerateText(ENGenerative-ai-inferenceLatestGenerateTextResultGenerateTextRequest) eNGenerative-ai-inferenceLatestGenerateTextResultGenerateText}, {@link #eNGenerative-ai-inferenceLatestSummarizeTextResultSummarizeText(ENGenerative-ai-inferenceLatestSummarizeTextResultSummarizeTextRequest) eNGenerative-ai-inferenceLatestSummarizeTextResultSummarizeText}, and {@link #eNGenerative-ai-inferenceLatestEmbedTextResultEmbedText(ENGenerative-ai-inferenceLatestEmbedTextResultEmbedTextRequest) eNGenerative-ai-inferenceLatestEmbedTextResultEmbedText}.

To use a Generative AI custom model for inference, you must first create an endpoint for that model. Use the {@link #eNGenerative-aiLatest(ENGenerative-aiLatestRequest) eNGenerative-aiLatest} to {@link #eNGenerative-aiLatestModel(ENGenerative-aiLatestModelRequest) eNGenerative-aiLatestModel} by fine-tuning an out-of-the-box model, or a previous version of a custom model, using your own data. Fine-tune the custom model on a {@link #eNGenerative-aiLatestDedicatedAiCluster(ENGenerative-aiLatestDedicatedAiClusterRequest) eNGenerative-aiLatestDedicatedAiCluster}. Then, create a {@link #eNGenerative-aiLatestDedicatedAiCluster(ENGenerative-aiLatestDedicatedAiClusterRequest) eNGenerative-aiLatestDedicatedAiCluster} with an {@link Endpoint} to host your custom model. For resource management in the Generative AI service, use the {@link #eNGenerative-aiLatest(ENGenerative-aiLatestRequest) eNGenerative-aiLatest}.

To learn more about the service, see the [Generative AI documentation](https://docs.oracle.com/iaas/Content/generative-ai/home.htm).

 * OpenAPI spec version: 20231130
 *
 *
 * NOTE: This class is auto generated by OracleSDKGenerator.
 * Do not edit the class manually.
 *
 * Copyright (c) 2020, 2025, Oracle and/or its affiliates.  All rights reserved.
 * This software is dual-licensed to you under the Universal Permissive License (UPL) 1.0 as shown at https://oss.oracle.com/licenses/upl or Apache License 2.0 as shown at http://www.apache.org/licenses/LICENSE-2.0. You may choose either license.
 */
var __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    Object.defineProperty(o, k2, { enumerable: true, get: function() { return m[k]; } });
}) : (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    o[k2] = m[k];
}));
var __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {
    Object.defineProperty(o, "default", { enumerable: true, value: v });
}) : function(o, v) {
    o["default"] = v;
});
var __importStar = (this && this.__importStar) || function (mod) {
    if (mod && mod.__esModule) return mod;
    var result = {};
    if (mod != null) for (var k in mod) if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);
    __setModuleDefault(result, mod);
    return result;
};
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.GenerativeAiInferenceClient = exports.GenerativeAiInferenceApiKeys = void 0;
const common = require("oci-common");
const model = __importStar(require("./model"));
const oci_common_1 = require("oci-common");
const Breaker = require("opossum");
// ===============================================
// This file is autogenerated - Please do not edit
// ===============================================
var GenerativeAiInferenceApiKeys;
(function (GenerativeAiInferenceApiKeys) {
})(GenerativeAiInferenceApiKeys = exports.GenerativeAiInferenceApiKeys || (exports.GenerativeAiInferenceApiKeys = {}));
/**
 * This service client uses {@link common.CircuitBreaker.DefaultConfiguration} for all the operations by default if no circuit breaker configuration is defined by the user.
 */
class GenerativeAiInferenceClient {
    constructor(params, clientConfiguration) {
        this["_realmSpecificEndpointTemplateEnabled"] = undefined;
        this["_endpoint"] = "";
        this["_defaultHeaders"] = {};
        this._circuitBreaker = null;
        this._httpOptions = undefined;
        this._bodyDuplexMode = undefined;
        this.targetService = "GenerativeAiInference";
        this._regionId = "";
        this._lastSetRegionOrRegionId = "";
        const requestSigner = params.authenticationDetailsProvider
            ? new common.DefaultRequestSigner(params.authenticationDetailsProvider)
            : null;
        this._authProvider = params.authenticationDetailsProvider;
        if (clientConfiguration) {
            this._clientConfiguration = clientConfiguration;
            this._circuitBreaker = clientConfiguration.circuitBreaker
                ? clientConfiguration.circuitBreaker.circuit
                : null;
            this._httpOptions = clientConfiguration.httpOptions
                ? clientConfiguration.httpOptions
                : undefined;
            this._bodyDuplexMode = clientConfiguration.bodyDuplexMode
                ? clientConfiguration.bodyDuplexMode
                : undefined;
        }
        if (!oci_common_1.developerToolConfiguration.isServiceEnabled("generativeaiinference")) {
            let errmsg = "The developerToolConfiguration configuration disabled this service, this behavior is controlled by developerToolConfiguration.ociEnabledServiceSet variable. Please check if your local developer_tool_configuration file has configured the service you're targeting or contact the cloud provider on the availability of this service : ";
            throw errmsg.concat("generativeaiinference");
        }
        // if circuit breaker is not created, check if circuit breaker system is enabled to use default circuit breaker
        const specCircuitBreakerEnabled = true;
        if (!this._circuitBreaker &&
            common.utils.isCircuitBreakerSystemEnabled(clientConfiguration) &&
            (specCircuitBreakerEnabled || common.CircuitBreaker.DefaultCircuitBreakerOverriden)) {
            this._circuitBreaker = new common.CircuitBreaker().circuit;
        }
        this._httpClient =
            params.httpClient ||
                new common.FetchHttpClient(requestSigner, this._circuitBreaker, this._httpOptions, this._bodyDuplexMode);
        if (params.authenticationDetailsProvider &&
            common.isRegionProvider(params.authenticationDetailsProvider)) {
            const provider = params.authenticationDetailsProvider;
            if (provider.getRegion()) {
                this.region = provider.getRegion();
            }
        }
    }
    /**
     * Get the endpoint that is being used to call (ex, https://www.example.com).
     */
    get endpoint() {
        return this._endpoint;
    }
    /**
     * Sets the endpoint to call (ex, https://www.example.com).
     * @param endpoint The endpoint of the service.
     */
    set endpoint(endpoint) {
        this._endpoint = endpoint;
        this._endpoint = this._endpoint + "/20231130";
        if (this.logger)
            this.logger.info(`GenerativeAiInferenceClient endpoint set to ${this._endpoint}`);
    }
    get logger() {
        return common.LOG.logger;
    }
    /**
     * Determines whether realm specific endpoint should be used or not.
     * Set realmSpecificEndpointTemplateEnabled to "true" if the user wants to enable use of realm specific endpoint template, otherwise set it to "false"
     * @param realmSpecificEndpointTemplateEnabled flag to enable the use of realm specific endpoint template
     */
    set useRealmSpecificEndpointTemplate(realmSpecificEndpointTemplateEnabled) {
        this._realmSpecificEndpointTemplateEnabled = realmSpecificEndpointTemplateEnabled;
        if (this.logger)
            this.logger.info(`realmSpecificEndpointTemplateEnabled set to ${this._realmSpecificEndpointTemplateEnabled}`);
        if (this._lastSetRegionOrRegionId === common.Region.REGION_STRING) {
            this.endpoint = common.EndpointBuilder.createEndpointFromRegion(GenerativeAiInferenceClient.serviceEndpointTemplate, this._region, GenerativeAiInferenceClient.endpointServiceName);
        }
        else if (this._lastSetRegionOrRegionId === common.Region.REGION_ID_STRING) {
            this.endpoint = common.EndpointBuilder.createEndpointFromRegionId(GenerativeAiInferenceClient.serviceEndpointTemplate, this._regionId, GenerativeAiInferenceClient.endpointServiceName);
        }
    }
    /**
     * Sets the region to call (ex, Region.US_PHOENIX_1).
     * Note, this will call {@link #endpoint(String) endpoint} after resolving the endpoint.
     * @param region The region of the service.
     */
    set region(region) {
        this._region = region;
        this.endpoint = common.EndpointBuilder.createEndpointFromRegion(GenerativeAiInferenceClient.serviceEndpointTemplate, region, GenerativeAiInferenceClient.endpointServiceName);
        this._lastSetRegionOrRegionId = common.Region.REGION_STRING;
    }
    /**
     * Sets the regionId to call (ex, 'us-phoenix-1').
     *
     * Note, this will first try to map the region ID to a known Region and call {@link #region(Region) region}.
     * If no known Region could be determined, it will create an endpoint assuming its in default Realm OC1
     * and then call {@link #endpoint(String) endpoint}.
     * @param regionId The public region ID.
     */
    set regionId(regionId) {
        this._regionId = regionId;
        this.endpoint = common.EndpointBuilder.createEndpointFromRegionId(GenerativeAiInferenceClient.serviceEndpointTemplate, regionId, GenerativeAiInferenceClient.endpointServiceName);
        this._lastSetRegionOrRegionId = common.Region.REGION_ID_STRING;
    }
    /**
     * Shutdown the circuit breaker used by the client when it is no longer needed
     */
    shutdownCircuitBreaker() {
        if (this._circuitBreaker) {
            this._circuitBreaker.shutdown();
        }
    }
    /**
     * Close the provider if possible which in turn shuts down any associated circuit breaker
     */
    closeProvider() {
        if (this._authProvider) {
            if (this._authProvider instanceof common.AbstractRequestingAuthenticationDetailsProvider)
                (this._authProvider).closeProvider();
        }
    }
    /**
     * Close the client once it is no longer needed
     */
    close() {
        this.shutdownCircuitBreaker();
        this.closeProvider();
    }
    /**
     * Applies guardrails to the input text, including content moderation, PII detection, and prompt injection protection.
     *
     * This operation uses {@link common.OciSdkDefaultRetryConfiguration} by default if no retry configuration is defined by the user.
     * @param ApplyGuardrailsRequest
     * @return ApplyGuardrailsResponse
     * @throws OciError when an error occurs
     * @example Click {@link https://docs.oracle.com/en-us/iaas/tools/typescript-sdk-examples/latest/generativeaiinference/ApplyGuardrails.ts.html |here} to see how to use ApplyGuardrails API.
     */
    applyGuardrails(applyGuardrailsRequest) {
        return __awaiter(this, void 0, void 0, function* () {
            if (this.logger)
                this.logger.debug("Calling operation GenerativeAiInferenceClient#applyGuardrails.");
            const operationName = "applyGuardrails";
            const apiReferenceLink = "";
            const pathParams = {};
            const queryParams = {};
            let headerParams = {
                "Content-Type": common.Constants.APPLICATION_JSON,
                "opc-retry-token": applyGuardrailsRequest.opcRetryToken,
                "opc-request-id": applyGuardrailsRequest.opcRequestId
            };
            const specRetryConfiguration = common.OciSdkDefaultRetryConfiguration;
            const retrier = oci_common_1.GenericRetrier.createPreferredRetrier(this._clientConfiguration ? this._clientConfiguration.retryConfiguration : undefined, applyGuardrailsRequest.retryConfiguration, specRetryConfiguration);
            if (this.logger)
                retrier.logger = this.logger;
            const request = yield oci_common_1.composeRequest({
                baseEndpoint: this._endpoint,
                defaultHeaders: this._defaultHeaders,
                path: "/actions/applyGuardrails",
                method: "POST",
                bodyContent: common.ObjectSerializer.serialize(applyGuardrailsRequest.applyGuardrailsDetails, "ApplyGuardrailsDetails", model.ApplyGuardrailsDetails.getJsonObj),
                pathParams: pathParams,
                headerParams: headerParams,
                queryParams: queryParams
            });
            try {
                const response = yield retrier.makeServiceCall(this._httpClient, request, this.targetService, operationName, apiReferenceLink);
                const sdkResponse = oci_common_1.composeResponse({
                    responseObject: {},
                    body: yield response.json(),
                    bodyKey: "applyGuardrailsResult",
                    bodyModel: model.ApplyGuardrailsResult,
                    type: "model.ApplyGuardrailsResult",
                    responseHeaders: [
                        {
                            value: response.headers.get("opc-request-id"),
                            key: "opcRequestId",
                            dataType: "string"
                        }
                    ]
                });
                return sdkResponse;
            }
            catch (err) {
                throw err;
            }
        });
    }
    /**
     * Creates a response for the given conversation.
     *
     * This operation uses {@link common.OciSdkDefaultRetryConfiguration} by default if no retry configuration is defined by the user.
     * @param ChatRequest
     * @return ChatResponse
     * @throws OciError when an error occurs
     * @example Click {@link https://docs.oracle.com/en-us/iaas/tools/typescript-sdk-examples/latest/generativeaiinference/Chat.ts.html |here} to see how to use Chat API.
     */
    chat(chatRequest) {
        return __awaiter(this, void 0, void 0, function* () {
            if (this.logger)
                this.logger.debug("Calling operation GenerativeAiInferenceClient#chat.");
            const operationName = "chat";
            const apiReferenceLink = "";
            const pathParams = {};
            const queryParams = {};
            let headerParams = {
                "Content-Type": common.Constants.APPLICATION_JSON,
                "opc-retry-token": chatRequest.opcRetryToken,
                "opc-request-id": chatRequest.opcRequestId
            };
            const specRetryConfiguration = common.OciSdkDefaultRetryConfiguration;
            const retrier = oci_common_1.GenericRetrier.createPreferredRetrier(this._clientConfiguration ? this._clientConfiguration.retryConfiguration : undefined, chatRequest.retryConfiguration, specRetryConfiguration);
            if (this.logger)
                retrier.logger = this.logger;
            const request = yield oci_common_1.composeRequest({
                baseEndpoint: this._endpoint,
                defaultHeaders: this._defaultHeaders,
                path: "/actions/chat",
                method: "POST",
                bodyContent: common.ObjectSerializer.serialize(chatRequest.chatDetails, "ChatDetails", model.ChatDetails.getJsonObj),
                pathParams: pathParams,
                headerParams: headerParams,
                queryParams: queryParams
            });
            try {
                const response = yield retrier.makeServiceCall(this._httpClient, request, this.targetService, operationName, apiReferenceLink);
                if (response.headers &&
                    response.headers.get(common.Constants.CONTENT_TYPE_HEADER) ===
                        common.Constants.SERVER_SIDE_EVENT_TEXT_STREAM) {
                    return response.body;
                }
                const sdkResponse = oci_common_1.composeResponse({
                    responseObject: {},
                    body: yield response.json(),
                    bodyKey: "chatResult",
                    bodyModel: model.ChatResult,
                    type: "model.ChatResult",
                    responseHeaders: [
                        {
                            value: response.headers.get("etag"),
                            key: "etag",
                            dataType: "string"
                        },
                        {
                            value: response.headers.get("opc-request-id"),
                            key: "opcRequestId",
                            dataType: "string"
                        },
                        {
                            value: response.headers.get("model-deprecation-info"),
                            key: "modelDeprecationInfo",
                            dataType: "string"
                        }
                    ]
                });
                return sdkResponse;
            }
            catch (err) {
                throw err;
            }
        });
    }
    /**
       * Produces embeddings for the inputs.
  * <p>
  An embedding is numeric representation of a piece of text. This text can be a phrase, a sentence, or one or more paragraphs. The Generative AI embedding model transforms each phrase, sentence, or paragraph that you input, into an array with 1024 numbers. You can use these embeddings for finding similarity in your input text such as finding phrases that are similar in context or category. Embeddings are mostly used for semantic searches where the search function focuses on the meaning of the text that it's searching through rather than finding results based on keywords.
  *
       * This operation uses {@link common.OciSdkDefaultRetryConfiguration} by default if no retry configuration is defined by the user.
       * @param EmbedTextRequest
       * @return EmbedTextResponse
       * @throws OciError when an error occurs
       * @example Click {@link https://docs.oracle.com/en-us/iaas/tools/typescript-sdk-examples/latest/generativeaiinference/EmbedText.ts.html |here} to see how to use EmbedText API.
       */
    embedText(embedTextRequest) {
        return __awaiter(this, void 0, void 0, function* () {
            if (this.logger)
                this.logger.debug("Calling operation GenerativeAiInferenceClient#embedText.");
            const operationName = "embedText";
            const apiReferenceLink = "";
            const pathParams = {};
            const queryParams = {};
            let headerParams = {
                "Content-Type": common.Constants.APPLICATION_JSON,
                "opc-retry-token": embedTextRequest.opcRetryToken,
                "opc-request-id": embedTextRequest.opcRequestId
            };
            const specRetryConfiguration = common.OciSdkDefaultRetryConfiguration;
            const retrier = oci_common_1.GenericRetrier.createPreferredRetrier(this._clientConfiguration ? this._clientConfiguration.retryConfiguration : undefined, embedTextRequest.retryConfiguration, specRetryConfiguration);
            if (this.logger)
                retrier.logger = this.logger;
            const request = yield oci_common_1.composeRequest({
                baseEndpoint: this._endpoint,
                defaultHeaders: this._defaultHeaders,
                path: "/actions/embedText",
                method: "POST",
                bodyContent: common.ObjectSerializer.serialize(embedTextRequest.embedTextDetails, "EmbedTextDetails", model.EmbedTextDetails.getJsonObj),
                pathParams: pathParams,
                headerParams: headerParams,
                queryParams: queryParams
            });
            try {
                const response = yield retrier.makeServiceCall(this._httpClient, request, this.targetService, operationName, apiReferenceLink);
                const sdkResponse = oci_common_1.composeResponse({
                    responseObject: {},
                    body: yield response.json(),
                    bodyKey: "embedTextResult",
                    bodyModel: model.EmbedTextResult,
                    type: "model.EmbedTextResult",
                    responseHeaders: [
                        {
                            value: response.headers.get("etag"),
                            key: "etag",
                            dataType: "string"
                        },
                        {
                            value: response.headers.get("opc-request-id"),
                            key: "opcRequestId",
                            dataType: "string"
                        },
                        {
                            value: response.headers.get("model-deprecation-info"),
                            key: "modelDeprecationInfo",
                            dataType: "string"
                        }
                    ]
                });
                return sdkResponse;
            }
            catch (err) {
                throw err;
            }
        });
    }
    /**
     * Generates a text response based on the user prompt.
     *
     * This operation uses {@link common.OciSdkDefaultRetryConfiguration} by default if no retry configuration is defined by the user.
     * @param GenerateTextRequest
     * @return GenerateTextResponse
     * @throws OciError when an error occurs
     * @example Click {@link https://docs.oracle.com/en-us/iaas/tools/typescript-sdk-examples/latest/generativeaiinference/GenerateText.ts.html |here} to see how to use GenerateText API.
     */
    generateText(generateTextRequest) {
        return __awaiter(this, void 0, void 0, function* () {
            if (this.logger)
                this.logger.debug("Calling operation GenerativeAiInferenceClient#generateText.");
            const operationName = "generateText";
            const apiReferenceLink = "";
            const pathParams = {};
            const queryParams = {};
            let headerParams = {
                "Content-Type": common.Constants.APPLICATION_JSON,
                "opc-retry-token": generateTextRequest.opcRetryToken,
                "opc-request-id": generateTextRequest.opcRequestId
            };
            const specRetryConfiguration = common.OciSdkDefaultRetryConfiguration;
            const retrier = oci_common_1.GenericRetrier.createPreferredRetrier(this._clientConfiguration ? this._clientConfiguration.retryConfiguration : undefined, generateTextRequest.retryConfiguration, specRetryConfiguration);
            if (this.logger)
                retrier.logger = this.logger;
            const request = yield oci_common_1.composeRequest({
                baseEndpoint: this._endpoint,
                defaultHeaders: this._defaultHeaders,
                path: "/actions/generateText",
                method: "POST",
                bodyContent: common.ObjectSerializer.serialize(generateTextRequest.generateTextDetails, "GenerateTextDetails", model.GenerateTextDetails.getJsonObj),
                pathParams: pathParams,
                headerParams: headerParams,
                queryParams: queryParams
            });
            try {
                const response = yield retrier.makeServiceCall(this._httpClient, request, this.targetService, operationName, apiReferenceLink);
                if (response.headers &&
                    response.headers.get(common.Constants.CONTENT_TYPE_HEADER) ===
                        common.Constants.SERVER_SIDE_EVENT_TEXT_STREAM) {
                    return response.body;
                }
                const sdkResponse = oci_common_1.composeResponse({
                    responseObject: {},
                    body: yield response.json(),
                    bodyKey: "generateTextResult",
                    bodyModel: model.GenerateTextResult,
                    type: "model.GenerateTextResult",
                    responseHeaders: [
                        {
                            value: response.headers.get("etag"),
                            key: "etag",
                            dataType: "string"
                        },
                        {
                            value: response.headers.get("opc-request-id"),
                            key: "opcRequestId",
                            dataType: "string"
                        },
                        {
                            value: response.headers.get("model-deprecation-info"),
                            key: "modelDeprecationInfo",
                            dataType: "string"
                        }
                    ]
                });
                return sdkResponse;
            }
            catch (err) {
                throw err;
            }
        });
    }
    /**
       * Reranks the text responses based on the input documents and a prompt.
  * <p>
  Rerank assigns an index and a relevance score to each document, indicating which document is most related to the prompt.
  *
       * This operation uses {@link common.OciSdkDefaultRetryConfiguration} by default if no retry configuration is defined by the user.
       * @param RerankTextRequest
       * @return RerankTextResponse
       * @throws OciError when an error occurs
       * @example Click {@link https://docs.oracle.com/en-us/iaas/tools/typescript-sdk-examples/latest/generativeaiinference/RerankText.ts.html |here} to see how to use RerankText API.
       */
    rerankText(rerankTextRequest) {
        return __awaiter(this, void 0, void 0, function* () {
            if (this.logger)
                this.logger.debug("Calling operation GenerativeAiInferenceClient#rerankText.");
            const operationName = "rerankText";
            const apiReferenceLink = "";
            const pathParams = {};
            const queryParams = {};
            let headerParams = {
                "Content-Type": common.Constants.APPLICATION_JSON,
                "opc-retry-token": rerankTextRequest.opcRetryToken,
                "opc-request-id": rerankTextRequest.opcRequestId
            };
            const specRetryConfiguration = common.OciSdkDefaultRetryConfiguration;
            const retrier = oci_common_1.GenericRetrier.createPreferredRetrier(this._clientConfiguration ? this._clientConfiguration.retryConfiguration : undefined, rerankTextRequest.retryConfiguration, specRetryConfiguration);
            if (this.logger)
                retrier.logger = this.logger;
            const request = yield oci_common_1.composeRequest({
                baseEndpoint: this._endpoint,
                defaultHeaders: this._defaultHeaders,
                path: "/actions/rerankText",
                method: "POST",
                bodyContent: common.ObjectSerializer.serialize(rerankTextRequest.rerankTextDetails, "RerankTextDetails", model.RerankTextDetails.getJsonObj),
                pathParams: pathParams,
                headerParams: headerParams,
                queryParams: queryParams
            });
            try {
                const response = yield retrier.makeServiceCall(this._httpClient, request, this.targetService, operationName, apiReferenceLink);
                const sdkResponse = oci_common_1.composeResponse({
                    responseObject: {},
                    body: yield response.json(),
                    bodyKey: "rerankTextResult",
                    bodyModel: model.RerankTextResult,
                    type: "model.RerankTextResult",
                    responseHeaders: [
                        {
                            value: response.headers.get("etag"),
                            key: "etag",
                            dataType: "string"
                        },
                        {
                            value: response.headers.get("opc-request-id"),
                            key: "opcRequestId",
                            dataType: "string"
                        },
                        {
                            value: response.headers.get("model-deprecation-info"),
                            key: "modelDeprecationInfo",
                            dataType: "string"
                        }
                    ]
                });
                return sdkResponse;
            }
            catch (err) {
                throw err;
            }
        });
    }
    /**
     * Summarizes the input text.
     *
     * This operation uses {@link common.OciSdkDefaultRetryConfiguration} by default if no retry configuration is defined by the user.
     * @param SummarizeTextRequest
     * @return SummarizeTextResponse
     * @throws OciError when an error occurs
     * @example Click {@link https://docs.oracle.com/en-us/iaas/tools/typescript-sdk-examples/latest/generativeaiinference/SummarizeText.ts.html |here} to see how to use SummarizeText API.
     */
    summarizeText(summarizeTextRequest) {
        return __awaiter(this, void 0, void 0, function* () {
            if (this.logger)
                this.logger.debug("Calling operation GenerativeAiInferenceClient#summarizeText.");
            const operationName = "summarizeText";
            const apiReferenceLink = "";
            const pathParams = {};
            const queryParams = {};
            let headerParams = {
                "Content-Type": common.Constants.APPLICATION_JSON,
                "opc-retry-token": summarizeTextRequest.opcRetryToken,
                "opc-request-id": summarizeTextRequest.opcRequestId
            };
            const specRetryConfiguration = common.OciSdkDefaultRetryConfiguration;
            const retrier = oci_common_1.GenericRetrier.createPreferredRetrier(this._clientConfiguration ? this._clientConfiguration.retryConfiguration : undefined, summarizeTextRequest.retryConfiguration, specRetryConfiguration);
            if (this.logger)
                retrier.logger = this.logger;
            const request = yield oci_common_1.composeRequest({
                baseEndpoint: this._endpoint,
                defaultHeaders: this._defaultHeaders,
                path: "/actions/summarizeText",
                method: "POST",
                bodyContent: common.ObjectSerializer.serialize(summarizeTextRequest.summarizeTextDetails, "SummarizeTextDetails", model.SummarizeTextDetails.getJsonObj),
                pathParams: pathParams,
                headerParams: headerParams,
                queryParams: queryParams
            });
            try {
                const response = yield retrier.makeServiceCall(this._httpClient, request, this.targetService, operationName, apiReferenceLink);
                const sdkResponse = oci_common_1.composeResponse({
                    responseObject: {},
                    body: yield response.json(),
                    bodyKey: "summarizeTextResult",
                    bodyModel: model.SummarizeTextResult,
                    type: "model.SummarizeTextResult",
                    responseHeaders: [
                        {
                            value: response.headers.get("etag"),
                            key: "etag",
                            dataType: "string"
                        },
                        {
                            value: response.headers.get("opc-request-id"),
                            key: "opcRequestId",
                            dataType: "string"
                        },
                        {
                            value: response.headers.get("model-deprecation-info"),
                            key: "modelDeprecationInfo",
                            dataType: "string"
                        }
                    ]
                });
                return sdkResponse;
            }
            catch (err) {
                throw err;
            }
        });
    }
}
exports.GenerativeAiInferenceClient = GenerativeAiInferenceClient;
GenerativeAiInferenceClient.serviceEndpointTemplate = "https://inference.generativeai.{region}.oci.{secondLevelDomain}";
GenerativeAiInferenceClient.endpointServiceName = "";
//# sourceMappingURL=client.js.map